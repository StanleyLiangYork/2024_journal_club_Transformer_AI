{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGppPqPiefdcA6RJB594DD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StanleyLiangYork/2024_journal_club_Transformer_AI/blob/main/Scaled_dot_product_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements the scaled dot product attention for transformer building blocks"
      ],
      "metadata": {
        "id": "j-dsTbJcWh-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformer architecture follows an encoder-decoder structure.\n",
        "\n",
        "*   Encoder: maps an input sequence to a numeric continuous representations.\n",
        "*   Decoder: receives the ouput of the encoder + the output of the decoder at the previous step, then generates the output sequence\n",
        "*   Unlike RNN, transformer does not rely on recurrence and convolutions\n",
        "\n",
        "Both the encoder and the decoder rely on the multi-head attention, or scaled dot-product attention to remember long sequence patterns.\n",
        "\n",
        "The scaled dot-product attention receives these queries (Q), keys (K), and values (V) as inputs, then first it computes the dot-product of the queries with the keys. The result is subsequently scaled by the squared root of $d_{k}$, the dimensionality of the key values (# of columns), to get the attention score.\n",
        "The attention scores are fed into a softmax function to get a set of attention weights. The attention weights scale the values (V) through a weighted multiplication operation. The whole process can be expressed as an attention function defined below:\n",
        "\n",
        "$attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_{k}}}V) $\n",
        "\n",
        "Note that each multi-head attention block uses a scaled dot-product attention operation to merge the attention heads.\n",
        "\n",
        "The scaled dot-product attention at the decoder also applied a mask to the attention scores (prevent the decoder looking at the info in the front) before feeding them into the softmax function.\n"
      ],
      "metadata": {
        "id": "Hqg58SkcWtG3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U8KLhk9rWgE9"
      },
      "outputs": [],
      "source": [
        "from numpy import random\n",
        "from tensorflow import matmul, math, cast, float32\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.backend import softmax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq_length = 5  # Maximum length of the input sequence\n",
        "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
        "d_v = 64  # Dimensionality of the linearly projected values\n",
        "batch_size = 64  # Batch size from the training process"
      ],
      "metadata": {
        "id": "2Pfbh73abyON"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the Scaled-Dot Product Attention\n",
        "# Inherit from the Tensorflow Layer class\n",
        "\n",
        "class DotProductAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, queries, keys, values, d_k, mask=None):\n",
        "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
        "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
        "\n",
        "        # Apply mask to the attention scores\n",
        "        if mask is not None:\n",
        "            scores += -1e9 * mask\n",
        "\n",
        "        # Computing the weights by a softmax operation\n",
        "        weights = softmax(scores)\n",
        "\n",
        "        # Computing the attention by a weighted sum of the value vectors\n",
        "        return matmul(weights, values)"
      ],
      "metadata": {
        "id": "zFZ5rk2Ub3_v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = random.random((batch_size, input_seq_length, d_k))\n",
        "keys = random.random((batch_size, input_seq_length, d_k))\n",
        "values = random.random((batch_size, input_seq_length, d_v))"
      ],
      "metadata": {
        "id": "kJJWGrSOcIwK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dot-product_attention object\n",
        "attention = DotProductAttention()"
      ],
      "metadata": {
        "id": "WGArL5BfcLfN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_scores = attention(queries, keys, values, d_k)\n",
        "print(attention_scores.shape)\n",
        "print(attention_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU3N9t10cSlb",
        "outputId": "b5d40049-4ca1-4092-c419-3233c826d6fe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 5, 64)\n",
            "tf.Tensor(\n",
            "[[[0.36580184 0.4946162  0.396619   ... 0.574539   0.3585462  0.6239938 ]\n",
            "  [0.36968642 0.4919685  0.3965832  ... 0.5787249  0.3551683  0.6248512 ]\n",
            "  [0.3598659  0.4799033  0.39663547 ... 0.5955414  0.3411876  0.60285866]\n",
            "  [0.38125867 0.5037129  0.40927058 ... 0.58126813 0.34202945 0.6259441 ]\n",
            "  [0.3876143  0.5160778  0.40697813 ... 0.559788   0.3605237  0.6431714 ]]\n",
            "\n",
            " [[0.3741589  0.5075558  0.5138777  ... 0.72573763 0.5537499  0.4150493 ]\n",
            "  [0.39473665 0.5120391  0.5254037  ... 0.7333318  0.5548649  0.4134689 ]\n",
            "  [0.37159356 0.49106467 0.5198252  ... 0.71670765 0.5640816  0.42884263]\n",
            "  [0.39580414 0.5126434  0.5281597  ... 0.7302247  0.56063825 0.41995424]\n",
            "  [0.37776154 0.50595164 0.51373225 ... 0.72351795 0.55618274 0.42122775]]\n",
            "\n",
            " [[0.540817   0.43442604 0.5778113  ... 0.40964317 0.49428675 0.34082535]\n",
            "  [0.5585393  0.44039524 0.60080713 ... 0.40660763 0.47136644 0.33223078]\n",
            "  [0.5562529  0.44410172 0.5933695  ... 0.41243148 0.48775625 0.33052337]\n",
            "  [0.5743774  0.4503809  0.6059991  ... 0.41829675 0.48399332 0.32346988]\n",
            "  [0.5411976  0.40843529 0.5653958  ... 0.3908968  0.49294895 0.36880085]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.4495662  0.59519976 0.36259198 ... 0.52814686 0.5766041  0.38658702]\n",
            "  [0.41926366 0.5833052  0.37204528 ... 0.5158138  0.58281434 0.3817828 ]\n",
            "  [0.41070545 0.5900595  0.38459387 ... 0.52622324 0.5739842  0.3802184 ]\n",
            "  [0.43169633 0.580239   0.3778382  ... 0.51577127 0.5797651  0.3847636 ]\n",
            "  [0.43502426 0.5985715  0.36902773 ... 0.53269047 0.57522565 0.38519904]]\n",
            "\n",
            " [[0.4345968  0.66930085 0.7239683  ... 0.5579516  0.5712095  0.43973365]\n",
            "  [0.44921744 0.6582679  0.758502   ... 0.5752631  0.5696109  0.45449826]\n",
            "  [0.44558465 0.681032   0.7461637  ... 0.57454026 0.57589245 0.44006765]\n",
            "  [0.44793954 0.65102696 0.75809705 ... 0.57862985 0.58335865 0.43650332]\n",
            "  [0.44143206 0.65575475 0.741739   ... 0.56610966 0.5727981  0.4440445 ]]\n",
            "\n",
            " [[0.17714624 0.70216215 0.73131716 ... 0.42067143 0.66081214 0.5696019 ]\n",
            "  [0.17358117 0.702737   0.73596984 ... 0.43857476 0.6587637  0.56886727]\n",
            "  [0.16656193 0.70489633 0.7328575  ... 0.44098622 0.658121   0.5659312 ]\n",
            "  [0.16857603 0.7108037  0.7189474  ... 0.44732633 0.6626706  0.5621002 ]\n",
            "  [0.17202483 0.7086847  0.72751117 ... 0.43394834 0.6643872  0.5676922 ]]], shape=(64, 5, 64), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}